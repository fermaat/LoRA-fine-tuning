# LoRA-fine-tuning

# UltraFeedback Training Suite

This repository contains modular, research-focused code for training and evaluating Large Language Models (LLMs) on the [`HuggingFaceH4/ultrafeedback_binarized`](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. It supports methods like **SFT**, **DPO**, and **ORPO**, with optional integration of **LoRA (PEFT)** adapters.

---

## ğŸ“Œ About

This repo is designed for **experimentation with alignment objectives** on preference-based datasets. It enables quick prototyping and testing of various training methods on binary comparisons derived from human feedback.

It is not intended to be production-ready but rather a **research scaffold** for controlled experimentation.

---

## âœ¨ Features

- âœ… Support for SFT, DPO, ORPO training modes
- ğŸ§  Easy integration with HuggingFace models and tokenizers
- ğŸª¶ PEFT/LoRA fine-tuning support via `peft`
- ğŸ“Š Logging predictions and basic evaluation (BLEU, ROUGE, BERTScore)
- ğŸ” Lightweight and modular script structure

---

## ğŸ§± Technology Stack

- `transformers` (HuggingFace)
- `datasets`
- `peft`
- `evaluate`
- `torch` (MPS or CUDA)

---

## ğŸ“ Project Structure

```bash
LoRA-fine.tuning/
â”œâ”€â”€ results
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loading.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ plotting_utils.py
â”‚   â”œâ”€â”€ training.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ testing_nb.ipynb
â””â”€â”€ ultrafeedback_train_sft.json
```

---

## âš™ï¸ Setup

### Prerequisites

- Python 3.10+
- PyTorch (with MPS or CUDA)
- Git + virtualenv or conda

### Install dependencies

```bash
pip install -r requirements.txt
```

ğŸš€ Quickstart

Evaluate a model (SFT or fine-tuned)

```bash
python main.py
```

This will:

Load the model (google/gemma-2b or another HF ID)
Run predictions on ultrafeedback_binarized
Save metrics and predictions to outputs/
Modify settings in settings.py
default_model_id = "google/gemma-2b"
use_peft = True
peft_settings = {
"r": 8,
"lora_alpha": 16,
...
}
ğŸ§ª Training Modes (Coming/Available)

âš ï¸ SFT (train_sft.py)
âš ï¸ DPO (train_dpo.py) â€“ experimental
âš ï¸ ORPO (train_orpo.py) â€“ in progress
To train with LoRA, pass use_peft = True and customize your LoRA config in settings.py.
ğŸ“Š Evaluation

Run:

```bash
python main.py
```

Generates:

```bash
outputs/predictions.txt
outputs/metrics.json with BLEU, ROUGE, and BERTScore
```

ğŸ§  Dataset Info

We use:

HuggingFaceH4/ultrafeedback_binarized
Structure includes:

prompt, chosen, and rejected texts
Use-case: preference modeling, reward modeling, DPO training
ğŸ¤ Contributing

This is a research-oriented repo. Feel free to open issues or pull requests to add new training methods, logging utilities, or evaluation functions.

ğŸ“„ License

MIT License. See LICENSE for more details.

ğŸ“¬ Contact
For questions, collaborations, or feedback, feel free to reach out:

ğŸ“§ Email: fermaat.vl@gmail.com
ğŸ§‘â€ğŸ’» GitHub: [@fermaat](https://github.com/fermaat)
ğŸŒ [Website](https://fermaat.github.io)
